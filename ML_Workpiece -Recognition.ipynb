{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZSXHwHQPDOK",
        "outputId": "365e2402-5766-4bcc-e14a-1066c8bb58d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qCHfycy91EyUFzilBvxu8hYVYp0l6jCh\n",
            "To: /tmp/MidTerm_Dataset.zip\n",
            "100% 50.4M/50.4M [00:00<00:00, 337MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id '1qCHfycy91EyUFzilBvxu8hYVYp0l6jCh' --output /tmp/MidTerm_Dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "local_zip = '/tmp/MidTerm_Dataset.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "0VTzHgpWPNjx"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "%cd /tmp/MidTerm_Dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm12-QTGRRKh",
        "outputId": "34b0461e-6d62-4d0c-843b-3a79ebdc367c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp\n",
            "/tmp/MidTerm_Dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import keras\n",
        "from PIL import Image #PIL 提供處理image的模組\n",
        "from keras.models import Sequential #建立最簡單的線性模型 (Sequentia1），就是一層層往下執行，沒有分叉 (If)\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D #CNN 的捲積層和池化層\n",
        "from keras.utils import np_utils #用來後續將 Label 標籤轉為 one-hot-encoding\n",
        "from matplotlib import pyplot as plt #用來繪製優化過程曲線的套件\n"
      ],
      "metadata": {
        "id": "0hd8WlNzR19b"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_x_y_preprocess(datapath):\n",
        "  img_row, img_col = 28, 28 # 定義圖片大小\n",
        "  datapath = datapath # 訓練資料路徑\n",
        "  data_x = np.zeros ( (28,28) ).reshape (1, 28, 28) # 儲存圖片\n",
        "  pictureCount = 0 # 紀錄圖片張敟\n",
        "  data_y = [] #紀錄label\n",
        "  workpiece_class = 4 # 工件種類有10種\n",
        "  # 讀取image資料夾内所有檔案\n",
        "  for root, dirs, files in os.walk(datapath):\n",
        "    for f in files:\n",
        "      label = int (root.split(os.sep) [2]) #取得1abel\n",
        "      data_y.append(label)\n",
        "      fullpath = os.path.join(root, f) #取得檔案路徑\n",
        "      img = Image.open(fullpath) #開啟img\n",
        "      img = (np.array(img)/255).reshape(1, 28,28) #讀取資料時順便做正規化典reshape\n",
        "      data_x = np.vstack( (data_x, img) )\n",
        "      pictureCount += 1\n",
        "  data_x = np.delete(data_x, [0], 0) #删除一開始宣告的np.zeros\n",
        "  #調整資料格式（圖片張數，img_row, img_col, 色彩通道=1（灰階)）\n",
        "  data_x = data_x.reshape(pictureCount, img_row, img_col, 1)\n",
        "  data_y = np_utils.to_categorical(data_y, workpiece_class) #將1abe1轉為one-hot encoding\n",
        "  return data_x, data_y"
      ],
      "metadata": {
        "id": "tU0xEyuzWdbZ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNurYA8MFo9u",
        "outputId": "2aefafe1-0aab-4ace-a0c0-bbc3618230c1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#取得train_X和train_Y\n",
        "data_train_X, data_train_Y = data_x_y_preprocess (\"train_image\") #, data y preprocess (\"image\")\n",
        "#取得test_X和test_Y\n",
        "data_test_X, data_test_Y = data_x_y_preprocess (\"test_image\")"
      ],
      "metadata": {
        "id": "OsJWR_cMYs5w"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers.core.dense import activations\n",
        "#建立簡單的線性執行的模型\n",
        "model = Sequential()\n",
        "#建立卷積層，filter=32\n",
        "model.add (Conv2D (32, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)))\n",
        "#建立池化層，池化大小=2x2，取最大值\n",
        "model.add (MaxPooling2D (pool_size=(2, 2)))\n",
        "#建立卷積層，filter=64\n",
        "model.add (Conv2D (64, (3, 3), activation='relu'))\n",
        "#建立池化層，池化大小=2×2，取最大值\n",
        "model.add (MaxPooling2D (pool_size=(2, 2)))\n"
      ],
      "metadata": {
        "id": "SiXmjQlYh86K"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropout層隨機断開輸入神經元，用於防止過度擬合，断開比例：0.1\n",
        "model.add( Dropout(0.1) )\n",
        "# Flatten層把多維的輸入一維化，常用在從卷積層到全連接層的過渡。\n",
        "model.add( Flatten() )\n",
        "# Dropout層隨機断開比例：0.1\n",
        "model.add( Dropout(0.1) )\n",
        "# 全連接層： 128 /output\n",
        "model.add( Dense(128,activation='relu') )\n",
        "# Dropout層隨機斷開比例：0.25\n",
        "model.add( Dropout (0.25) )\n",
        "# 使用 softmax activation function，將結果分類 (units=4 表示分4類）\n",
        "model.add( Dense (units=4, activation='softmax') )"
      ],
      "metadata": {
        "id": "WWXL1hZDk8Ne"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary() # 查看模型架構  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaS3TcVHltKG",
        "outputId": "d41313e4-78fa-423c-ef33-4cffd5e50d3a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 13, 13, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 5, 5, 64)          0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               204928    \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 4)                 516       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 224,260\n",
            "Trainable params: 224,260\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 編譯：選擇損失函數、優化方法及成效衡量方式\n",
        "model.compile (loss=\"categorical _crossentropy\", optimizer=\"adam\", metrics=[' accuracy'])\n",
        "\n",
        "train_history = model.fit (data_train_X, data_train_Y, batch_size=32, epochs=100, verbose=1, validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "tDkaHGzJl9df",
        "outputId": "e157c58d-4e91-48eb-f57f-355cfe236b8f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-3fc71a85001c>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"categorical _crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m' accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata_train_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_train_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mtrain_validation_split\u001b[0;34m(arrays, validation_split)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msplit_at\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msplit_at\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbatch_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1687\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   1688\u001b[0m             \u001b[0;34m\"Training data contains {batch_dim} samples, which is not \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m             \u001b[0;34m\"sufficient to split it into a validation and training set as \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Training data contains 0 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.1`. Either provide more data, or a different value for the `validation_split` argument."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HS17hZGfrACU"
      }
    }
  ]
}